{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "text-summarization-and-sentiment-analysis.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxrYG_IJjbYC",
        "outputId": "115879de-de16-40a2-c7b3-af76fa1439c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install transformers\n",
        "pip install sumy"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sumy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/20/8abf92617ec80a2ebaec8dc1646a790fc9656a4a4377ddb9f0cc90bc9326/sumy-0.8.1-py2.py3-none-any.whl (83kB)\n",
            "\r\u001b[K     |████                            | 10kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 20kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 30kB 9.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 40kB 8.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 51kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 61kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 71kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 81kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 3.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from sumy) (0.6.2)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from sumy) (2.23.0)\n",
            "Collecting pycountry>=18.2.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/73/6f1a412f14f68c273feea29a6ea9b9f1e268177d32e0e69ad6790d306312/pycountry-20.7.3.tar.gz (10.1MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1MB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from sumy) (3.2.5)\n",
            "Collecting breadability>=0.1.20\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/2d/bb6c9b381e6b6a432aa2ffa8f4afdb2204f1ff97cfcc0766a5b7683fec43/breadability-0.1.20.tar.gz\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->sumy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->sumy) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->sumy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.7.0->sumy) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.0.2->sumy) (1.15.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.7/dist-packages (from breadability>=0.1.20->sumy) (4.2.6)\n",
            "Building wheels for collected packages: pycountry, breadability\n",
            "  Building wheel for pycountry (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycountry: filename=pycountry-20.7.3-py2.py3-none-any.whl size=10746863 sha256=174b185bc03f009bfe59a1fe05544ad94e68c4329e62c1720e92dedc2fb33924\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/4e/a6/be297e6b83567e537bed9df4a93f8590ec01c1acfbcd405348\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21680 sha256=fe83044389b884854c150a27b0f81eb621bfe1df951c1835fefee99ececcfd1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/4d/a1/510b12c5e65e0b2b3ce539b2af66da0fc57571e528924f4a52\n",
            "Successfully built pycountry breadability\n",
            "Installing collected packages: pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 pycountry-20.7.3 sumy-0.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-kN3bLpdHnB",
        "outputId": "522b1cdf-754d-4ce7-b84d-9e7ce8e0f65e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import json\n",
        "import requests\n",
        "import nltk\n",
        "import sys\n",
        "import re\n",
        "import heapq\n",
        "import gensim\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "from math import ceil\n",
        "\n",
        "from gensim.summarization import summarize"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uP6bPwRYdHnH"
      },
      "source": [
        "logging = True"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOo11T1edHnH"
      },
      "source": [
        "def read_reviews():\n",
        "    raw = requests.get(\"https://raw.githubusercontent.com/patelviralb/text-summarization/main/dataset/cornell_reviews.json\").text.strip()\n",
        "    corpus = [json.loads(line) for line in raw.split(\"\\n\")]\n",
        "    \n",
        "    return corpus"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g2_F57ZdHnH"
      },
      "source": [
        "def vectorize_input_corpus(documents):\n",
        "    documents = []\n",
        "    classes = []\n",
        "    stop_words = stopwords.words('english')\n",
        "\n",
        "    for entry in corpus:\n",
        "        documents.append(entry['text'])\n",
        "        classes.append(entry['class'])\n",
        "\n",
        "    vectorizer = CountVectorizer(input=documents, max_df=0.25, token_pattern=r'\\b[a-zA-Z0-9]*[a-zA-Z][a-zA-Z0-9]*\\b', ngram_range=(1,3), max_features=300000, binary=True)\n",
        "    count_vector = vectorizer.fit_transform(documents)\n",
        "\n",
        "    vectors = count_vector.toarray()\n",
        "#     vocab = vectorizer.get_feature_names()\n",
        "\n",
        "#     return vectors, classes, vocab\n",
        "    return vectors, documents, classes"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NMUmUuvdHnI"
      },
      "source": [
        "def distribute_test_train_corpus(vectors, documents, classes):\n",
        "    document_indices = [*range(0, len(documents), 1)]\n",
        "    test_train_data_indces = train_test_split(document_indices, train_size = 0.75, random_state = 41)\n",
        "\n",
        "    train_vectors = []\n",
        "    train_documents = []\n",
        "    train_classes = []\n",
        "\n",
        "    for index in test_train_data_indces[0]:\n",
        "        train_vectors.append(vectors[index])\n",
        "        train_documents.append(documents[index])\n",
        "        train_classes.append(classes[index])\n",
        "\n",
        "    test_vectors = []\n",
        "    test_documents = []\n",
        "    test_classes = []\n",
        "\n",
        "    for index in test_train_data_indces[1]:\n",
        "        test_vectors.append(vectors[index])\n",
        "        test_documents.append(documents[index])\n",
        "        test_classes.append(classes[index])\n",
        "    \n",
        "    return train_vectors, train_documents, train_classes, test_vectors, test_documents, test_classes"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lohrvUtJdHnI"
      },
      "source": [
        "def get_model(train_vectors, train_classes):\n",
        "    logistic_regression_model = LogisticRegression(C=0.05, solver='liblinear', max_iter = 1000, penalty=\"l2\")\n",
        "    logistic_regression_model.fit(train_vectors, train_classes)\n",
        "    \n",
        "    return logistic_regression_model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJkg9OdDdHnI"
      },
      "source": [
        "def run_evaluation(model, test_vectors, test_classes):\n",
        "    accuracy = accuracy_score(test_classes, model.predict(test_vectors))\n",
        "\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvZI3NirdHnJ"
      },
      "source": [
        "### Initial Accuracy Computation\n",
        "\n",
        "Below code computes the baseline accuracy after dividing the corpus into training and test dataset. This accuracy will be used to compare with the accuracies generated after summarization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9V3mLLldHnJ"
      },
      "source": [
        "corpus = read_reviews()\n",
        "vectors, documents, classes = vectorize_input_corpus(corpus)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mac07sMdHnJ"
      },
      "source": [
        "train_vectors, train_documents, train_classes, test_vectors, test_documents, test_classes = distribute_test_train_corpus(vectors, documents, classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HjZUwq7dHnJ",
        "outputId": "e89fb5c8-fbfa-4ca9-c840-104aa886dd86"
      },
      "source": [
        "logistic_regression_model = get_model(train_vectors, train_classes)\n",
        "baseline_accuracy = run_evaluation(logistic_regression_model, test_vectors, test_classes)\n",
        "\n",
        "print(\"baseline_accuracy:\\t{}\".format(baseline_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "baseline_accuracy:\t0.896\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u59mtyVodHnK"
      },
      "source": [
        "## Summarization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caA3eMZgdHnK"
      },
      "source": [
        "def vectorize_summary(summary_corpus):\n",
        "    vectorizer = CountVectorizer(input=summary_corpus, max_df=0.25, token_pattern=r'\\b[a-zA-Z0-9]*[a-zA-Z][a-zA-Z0-9]*\\b', ngram_range=(1,3), max_features=300000, binary=True)\n",
        "    count_vector = vectorizer.fit_transform(summary_corpus)\n",
        "\n",
        "    summary_vectors = count_vector.toarray()\n",
        "    \n",
        "    return summary_vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDP4l04wdHnK"
      },
      "source": [
        "### Summarize Text using `Weighted Word Frequency`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsIguUCqdHnK"
      },
      "source": [
        "def get_weighted_word_summary(document, summary_sentence_count = 10, max_words_in_sentence = sys.maxsize):\n",
        "    original_text = document\n",
        "    # Preprocessing\n",
        "    formatted_text = re.sub(r'\\s+', ' ',  re.sub('[^a-zA-Z]', ' ', document))\n",
        "    # Converting Text To Sentences\n",
        "    sentence_list = sent_tokenize(document)\n",
        "    \n",
        "    # Find Weighted Frequency of Occurrence\n",
        "    stop_words = stopwords.words('english')\n",
        "    word_frequencies = {}\n",
        "    for word in word_tokenize(formatted_text):\n",
        "        if word not in stop_words:\n",
        "            if word not in word_frequencies.keys():\n",
        "                word_frequencies[word] = 1\n",
        "            else:\n",
        "                word_frequencies[word] += 1\n",
        "    \n",
        "    maximum_frequncy = max(word_frequencies.values())\n",
        "    for word in word_frequencies.keys():\n",
        "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
        "    \n",
        "    # Calculating Sentence Scores\n",
        "    sentence_scores = {}\n",
        "    for sentence in sentence_list:\n",
        "        if len(sentence.split(' ')) < max_words_in_sentence:\n",
        "            for word in word_tokenize(sentence.lower()):\n",
        "                if word in word_frequencies.keys():\n",
        "                    if sentence not in sentence_scores.keys():\n",
        "                        sentence_scores[sentence] = word_frequencies[word]\n",
        "                    else:\n",
        "                        sentence_scores[sentence] += word_frequencies[word]\n",
        "    \n",
        "    # Getting the Summary\n",
        "    summary_sentences = heapq.nlargest(summary_sentence_count, sentence_scores, key=sentence_scores.get)\n",
        "    summarized_text = ' '.join(summary_sentences)\n",
        "    \n",
        "    return summarized_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW3DHh6gdHnL"
      },
      "source": [
        "def create_weighted_word_summary_corpus(documents, summary_sentence_count = 10, max_words_in_sentence = sys.maxsize):\n",
        "    document_summary = []\n",
        "    for index, document in enumerate(documents):\n",
        "        summary = get_weighted_word_summary(document, summary_sentence_count, max_words_in_sentence)\n",
        "        document_summary.append(summary)\n",
        "    \n",
        "    return document_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVTjYOPmdHnL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFIk1Ra1dHnM",
        "outputId": "86e330c8-091d-4ffb-daab-9c98c8c30d79"
      },
      "source": [
        "# summary_sentence_count = 7, max_words_in_sentence = 30\n",
        "weighted_word_summary_accuracy = compute_weighted_word_summary_accuracy(train_documents, train_classes, test_documents, test_classes, summary_sentence_count = 7, max_words_in_sentence = 30)\n",
        "print(\"weighted_word_summary_accuracy:\\t{}\".format(weighted_word_summary_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weighted_word_summary_accuracy:\t0.742\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2dvx5R1dHnM",
        "outputId": "58c25ebc-2d22-4703-e6ee-78861827324e"
      },
      "source": [
        "# summary_sentence_count = 7\n",
        "weighted_word_summary_accuracy = compute_weighted_word_summary_accuracy(train_documents, train_classes, test_documents, test_classes, summary_sentence_count = 7)\n",
        "print(\"weighted_word_summary_accuracy:\\t{}\".format(weighted_word_summary_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weighted_word_summary_accuracy:\t0.762\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxgaGkNqdHnM",
        "outputId": "39e9efc0-c6f7-4648-c3a7-47d29e7854aa"
      },
      "source": [
        "weighted_word_summary_accuracy = compute_weighted_word_summary_accuracy(train_documents, train_classes, test_documents, test_classes)\n",
        "print(\"weighted_word_summary_accuracy:\\t{}\".format(weighted_word_summary_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weighted_word_summary_accuracy:\t0.784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQDOlPoLdHnM"
      },
      "source": [
        "### Summarize Text using `transformers.pipeline`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga19LOn1dHnM"
      },
      "source": [
        "def get_transformers_pipeline_summary(original_text, minimum_length = 20, maximum_length = 200):\n",
        "    original_text_length = len(original_text)\n",
        "    \n",
        "    summarization = pipeline(\"summarization\")\n",
        "    \n",
        "    original_documents = []\n",
        "    if original_text_length > 1024:\n",
        "        total_range = ceil(original_text_length / 1024)\n",
        "        i = 0\n",
        "        while i < total_range:\n",
        "            start = i * 1024\n",
        "            end = (start + 1024) if i != total_range - 1 else original_text_length\n",
        "            summarized_text = summarization(original_text[start:end], min_length = minimum_length, max_length = original_text_length if maximum_length > original_text_length else maximum_length)[0]['summary_text']\n",
        "            original_documents.append(summarized_text)\n",
        "            i += 1\n",
        "        \n",
        "        return \" \".join(original_documents)\n",
        "    \n",
        "    summarized_text = summarization(original_text, min_length = minimum_length, max_length = maximum_length)[0]['summary_text']\n",
        "    \n",
        "    return summarized_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekDJVFIZdHnN"
      },
      "source": [
        "def create_transformers_pipeline_summary_corpus(documents, min_length = 20, max_length = 200):\n",
        "    document_summary = []\n",
        "    for index, document in enumerate(documents):\n",
        "        summary = get_transformers_pipeline_summary(document, min_length, max_length)\n",
        "        document_summary.append(summary)\n",
        "    \n",
        "    return document_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5Xv3im7dHnN"
      },
      "source": [
        "def compute_transformers_pipeline_summary_accuracy(train_documents, train_classes, test_documents, test_classes, min_length = 20, max_length = 200):\n",
        "    train_summary = create_transformers_pipeline_summary_corpus(train_documents, min_length, max_length)\n",
        "    if logging == True: print(\"train_summary completed\")\n",
        "    test_summary = create_transformers_pipeline_summary_corpus(test_documents, min_length, max_length)\n",
        "    if logging == True: print(\"test_summary completed\")\n",
        "    \n",
        "    summary_corpus = []\n",
        "    summary_corpus.extend(train_summary)\n",
        "    summary_corpus.extend(test_summary)\n",
        "    if logging == True: print(\"summary_corpus created\")\n",
        "\n",
        "    summary_vectors = vectorize_summary(summary_corpus)\n",
        "    train_summary_vectors = summary_vectors[0:1500]\n",
        "    test_summary_vectors = summary_vectors[1500:]\n",
        "    if logging == True: print(\"summary_corpus distributed\")\n",
        "\n",
        "    logistic_regression_model_after_summary = get_model(train_summary_vectors, train_classes)\n",
        "    if logging == True: print(\"logistic_regression_model_after_summary computed\")\n",
        "    transformers_pipeline_summary_accuracy = run_evaluation(logistic_regression_model_after_summary, test_summary_vectors, test_classes)\n",
        "    if logging == True: print(\"transformers_pipeline_summary_accuracy computed\")\n",
        "\n",
        "    return transformers_pipeline_summary_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7u15aMR1dHnN"
      },
      "source": [
        "# min_length = 5, max_length = 20\n",
        "# transformers_pipeline_summary_accuracy = compute_transformers_pipeline_summary_accuracy(train_documents, train_classes, test_documents, test_classes, 0, 20)\n",
        "# print(\"transformers_pipeline_summary_accuracy:\\t{}\".format(transformers_pipeline_summary_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3gSM_T3dHnN"
      },
      "source": [
        "# min_length = 20, max_length = 200\n",
        "# transformers_pipeline_summary_accuracy = compute_transformers_pipeline_summary_accuracy(train_documents, train_classes, test_documents, test_classes, 0, 200)\n",
        "# print(\"transformers_pipeline_summary_accuracy:\\t{}\".format(transformers_pipeline_summary_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_kq87VbdHnO"
      },
      "source": [
        "### Summarize Text using `TextRank`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmXpzQ8AdHnO"
      },
      "source": [
        "def get_textrank_summary(original_text, ratio):\n",
        "    try:\n",
        "        summarized_text = summarize(original_text, ratio)\n",
        "    except ValueError as v:\n",
        "        return original_text\n",
        "\n",
        "    return summarized_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QElZpYW5dHnO"
      },
      "source": [
        "def create_textrank_summary_corpus(documents, ratio):\n",
        "    document_summary = []\n",
        "    for index, document in enumerate(documents):\n",
        "        summary = get_textrank_summary(document, ratio)\n",
        "        document_summary.append(summary)\n",
        "    \n",
        "    return document_summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDS2XTxadHnO"
      },
      "source": [
        "def compute_textrank_summary_accuracy(train_documents, train_classes, test_documents, test_classes, ratio=0.2):\n",
        "    train_summary = create_textrank_summary_corpus(train_documents, ratio)\n",
        "    if logging == True: print(\"train_summary completed\")\n",
        "    test_summary = create_textrank_summary_corpus(test_documents, ratio)\n",
        "    if logging == True: print(\"test_summary completed\")\n",
        "    \n",
        "    summary_corpus = []\n",
        "    summary_corpus.extend(train_summary)\n",
        "    summary_corpus.extend(test_summary)\n",
        "    if logging == True: print(\"summary_corpus created\")\n",
        "\n",
        "    summary_vectors = vectorize_summary(summary_corpus)\n",
        "    train_summary_vectors = summary_vectors[0:1500]\n",
        "    test_summary_vectors = summary_vectors[1500:]\n",
        "    if logging == True: print(\"summary_corpus distributed\")\n",
        "\n",
        "    logistic_regression_model_after_summary = get_model(train_summary_vectors, train_classes)\n",
        "    if logging == True: print(\"logistic_regression_model_after_summary computed\")\n",
        "    transformers_pipeline_summary_accuracy = run_evaluation(logistic_regression_model_after_summary, test_summary_vectors, test_classes)\n",
        "    if logging == True: print(\"transformers_pipeline_summary_accuracy computed\")\n",
        "\n",
        "    return transformers_pipeline_summary_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi9Aopz1dHnO",
        "outputId": "575a9de2-49e9-44d4-eef1-ccb87c73e706"
      },
      "source": [
        "textrank_summary_accuray = compute_textrank_summary_accuracy(train_documents, train_classes, test_documents, test_classes, 0.2)\n",
        "print(\"textrank_summary_accuray:\\t{}\".format(textrank_summary_accuray))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_summary completed\n",
            "test_summary completed\n",
            "summary_corpus created\n",
            "summary_corpus distributed\n",
            "logistic_regression_model_after_summary computed\n",
            "transformers_pipeline_summary_accuracy computed\n",
            "textrank_summary_accuray:\t0.742\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnWdoR-vdHnP",
        "outputId": "f5642dd3-32cc-4c8a-a634-8ec16add133e"
      },
      "source": [
        "textrank_summary_accuray = compute_textrank_summary_accuracy(train_documents, train_classes, test_documents, test_classes, 0.3)\n",
        "print(\"textrank_summary_accuray:\\t{}\".format(textrank_summary_accuray))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_summary completed\n",
            "test_summary completed\n",
            "summary_corpus created\n",
            "summary_corpus distributed\n",
            "logistic_regression_model_after_summary computed\n",
            "transformers_pipeline_summary_accuracy computed\n",
            "textrank_summary_accuray:\t0.788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "qYYLBMkldHnP",
        "outputId": "e4fd0dc0-0671-454d-952b-428d96d772e9"
      },
      "source": [
        "textrank_summary_accuray = compute_textrank_summary_accuracy(train_documents, train_classes, test_documents, test_classes, 0.5)\n",
        "print(\"textrank_summary_accuray:\\t{}\".format(textrank_summary_accuray))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_summary completed\n",
            "test_summary completed\n",
            "summary_corpus created\n",
            "summary_corpus distributed\n",
            "logistic_regression_model_after_summary computed\n",
            "transformers_pipeline_summary_accuracy computed\n",
            "textrank_summary_accuray:\t0.818\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjO99UuOkiDE",
        "outputId": "ce268ab5-26d3-41a8-a2ac-5e6f4282e7a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "nltk.download('punkt')\n",
        "def lsaSummary(text):\n",
        "  summarizer_lsa = LsaSummarizer()\n",
        "\n",
        "  parser = PlaintextParser.from_string(text,Tokenizer(\"english\"))\n",
        "  summary =summarizer_lsa(parser.document,2)\n",
        "\n",
        "  text_summary=\"\"\n",
        "  for sentence in summary:\n",
        "      text_summary+=str(sentence)\n",
        "\n",
        "  return text_summary\n",
        "  # print(text_summary)\n",
        "  # print(\"----\")\n",
        "  # print(documents[2])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFKXS9cmoNNF"
      },
      "source": [
        "def genSumLSA(documents):\n",
        "    document_summary = []\n",
        "    i=0\n",
        "    for index, document in enumerate(documents):\n",
        "      print(i)\n",
        "      print(\"--\")\n",
        "      summary = lsaSummary(document)\n",
        "      document_summary.append(summary)\n",
        "      i+=1\n",
        "\n",
        "    \n",
        "    return document_summary\n",
        "\n",
        "result=genSumLSA(documents)\n",
        "\n",
        "print(result[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8LFAYTeuX1W"
      },
      "source": [
        "from sumy.summarizers.kl import KLSummarizer\n",
        "\n",
        "\n",
        "def klDiv(text):\n",
        "    \n",
        "  summarizer_kl = KLSummarizer()\n",
        "  parser = PlaintextParser.from_string(text,Tokenizer(\"english\"))\n",
        "  summary =summarizer_kl(parser.document,2)\n",
        "  kl_summary=\"\"\n",
        "  for sentence in summary:\n",
        "      kl_summary+=str(sentence)  \n",
        "  # print(kl_summary)\n",
        "  return kl_summary"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S17vGFOcu1X1"
      },
      "source": [
        "def genSumKL(documents):\n",
        "    document_summary = []\n",
        "    i=0\n",
        "    for index, document in enumerate(documents):\n",
        "      print(i)\n",
        "      print(\"--\")\n",
        "      summary = klDiv(document)\n",
        "      document_summary.append(summary)\n",
        "      i+=1\n",
        "\n",
        "    \n",
        "    return document_summary\n",
        "\n",
        "result=genSumKL(documents)\n",
        "\n",
        "print(result[0])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}