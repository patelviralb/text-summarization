{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\viral\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_reviews():\n",
    "    raw = requests.get(\"https://raw.githubusercontent.com/patelviralb/text-summarization/main/dataset/cornell_reviews.json\").text.strip()\n",
    "    corpus = [json.loads(line) for line in raw.split(\"\\n\")]\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_input(documents):\n",
    "    documents = []\n",
    "    classes = []\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    for entry in corpus:\n",
    "        documents.append(entry['text'])\n",
    "        classes.append(entry['class'])\n",
    "\n",
    "    vectorizer = CountVectorizer(input=documents, max_df=0.25, token_pattern=r'\\b[a-zA-Z0-9]*[a-zA-Z][a-zA-Z0-9]*\\b', ngram_range=(1,3), max_features=300000, binary=True)\n",
    "    count_vector = vectorizer.fit_transform(documents)\n",
    "\n",
    "    vectors = count_vector.toarray()\n",
    "#     vocab = vectorizer.get_feature_names()\n",
    "\n",
    "#     return vectors, classes, vocab\n",
    "    return vectors, documents, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_test_train_corpus(vectors, documents, classes):\n",
    "    document_indices = [*range(0, len(documents), 1)]\n",
    "    test_train_data_indces = train_test_split(document_indices, train_size = 0.75, random_state = 41)\n",
    "    # print(\"len(test_train_data_indces):\\t{}\".format(len(test_train_data_indces)))\n",
    "    # print(\"len(test_train_data_indces[0]):\\t{}\".format(len(test_train_data_indces[0])))\n",
    "    # print(\"len(test_train_data_indces[1]):\\t{}\".format(len(test_train_data_indces[1])))\n",
    "\n",
    "    train_vectors = []\n",
    "    train_documents = []\n",
    "    train_classes = []\n",
    "\n",
    "    for index in test_train_data_indces[0]:\n",
    "        train_vectors.append(vectors[index])\n",
    "        train_documents.append(documents[index])\n",
    "        train_classes.append(classes[index])\n",
    "\n",
    "    test_vectors = []\n",
    "    test_documents = []\n",
    "    test_classes = []\n",
    "\n",
    "    for index in test_train_data_indces[1]:\n",
    "        test_vectors.append(vectors[index])\n",
    "        test_documents.append(documents[index])\n",
    "        test_classes.append(classes[index])\n",
    "\n",
    "    # print(\"len(train_vectors):\\t{}\".format(len(train_vectors)))\n",
    "    # print(\"len(train_documents):\\t{}\".format(len(train_documents)))\n",
    "    # print(\"len(train_classes):\\t{}\".format(len(train_classes)))\n",
    "\n",
    "    # print(\"len(test_vectors):\\t{}\".format(len(test_vectors)))\n",
    "    # print(\"len(test_documents):\\t{}\".format(len(test_documents)))\n",
    "    # print(\"len(test_classes):\\t{}\".format(len(test_classes)))\n",
    "    \n",
    "    return train_vectors, train_documents, train_classes, test_vectors, test_documents, test_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(train_vectors, train_classes):\n",
    "    logistic_regression_model = LogisticRegression(C=0.05, solver='liblinear', max_iter = 500, penalty=\"l2\")\n",
    "    logistic_regression_model.fit(train_vectors, train_classes)\n",
    "    \n",
    "    return logistic_regression_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(model, test_vectors, test_classes):\n",
    "    accuracy = accuracy_score(test_classes, model.predict(test_vectors))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Accuracy Computation\n",
    "\n",
    "Below code computes the baseline accuracy after dividing the corpus into training and test dataset. This accuracy will be used to compare with the accuracies generated after summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_reviews()\n",
    "vectors, documents, classes = vectorize_input(corpus)\n",
    "\n",
    "# print(\"type(vectors):\\t{}\".format(type(vectors)))\n",
    "# print(\"type(documents):\\t{}\".format(type(documents)))\n",
    "# print(\"type(classes):\\t{}\".format(type(classes)))\n",
    "\n",
    "# print(\"len(vectors):\\t{}\".format(len(vectors)))\n",
    "# print(\"len(documents):\\t{}\".format(len(documents)))\n",
    "# print(\"len(classes):\\t{}\".format(len(classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors, train_documents, train_classes, test_vectors, test_documents, test_classes = distribute_test_train_corpus(vectors, documents, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_accuracy:\t0.896\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_model = get_model(train_vectors, train_classes)\n",
    "baseline_accuracy = run_evaluation(logistic_regression_model, test_vectors, test_classes)\n",
    "\n",
    "print(\"baseline_accuracy:\\t{}\".format(baseline_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
