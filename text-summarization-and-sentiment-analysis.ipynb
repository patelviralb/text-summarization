{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\viral\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import nltk\n",
    "import re\n",
    "import heapq\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_reviews():\n",
    "    raw = requests.get(\"https://raw.githubusercontent.com/patelviralb/text-summarization/main/dataset/cornell_reviews.json\").text.strip()\n",
    "    corpus = [json.loads(line) for line in raw.split(\"\\n\")]\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_input_corpus(documents):\n",
    "    documents = []\n",
    "    classes = []\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    for entry in corpus:\n",
    "        documents.append(entry['text'])\n",
    "        classes.append(entry['class'])\n",
    "\n",
    "    vectorizer = CountVectorizer(input=documents, max_df=0.25, token_pattern=r'\\b[a-zA-Z0-9]*[a-zA-Z][a-zA-Z0-9]*\\b', ngram_range=(1,3), max_features=300000, binary=True)\n",
    "    count_vector = vectorizer.fit_transform(documents)\n",
    "\n",
    "    vectors = count_vector.toarray()\n",
    "#     vocab = vectorizer.get_feature_names()\n",
    "\n",
    "#     return vectors, classes, vocab\n",
    "    return vectors, documents, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_test_train_corpus(vectors, documents, classes):\n",
    "    document_indices = [*range(0, len(documents), 1)]\n",
    "    test_train_data_indces = train_test_split(document_indices, train_size = 0.75, random_state = 41)\n",
    "\n",
    "    train_vectors = []\n",
    "    train_documents = []\n",
    "    train_classes = []\n",
    "\n",
    "    for index in test_train_data_indces[0]:\n",
    "        train_vectors.append(vectors[index])\n",
    "        train_documents.append(documents[index])\n",
    "        train_classes.append(classes[index])\n",
    "\n",
    "    test_vectors = []\n",
    "    test_documents = []\n",
    "    test_classes = []\n",
    "\n",
    "    for index in test_train_data_indces[1]:\n",
    "        test_vectors.append(vectors[index])\n",
    "        test_documents.append(documents[index])\n",
    "        test_classes.append(classes[index])\n",
    "    \n",
    "    return train_vectors, train_documents, train_classes, test_vectors, test_documents, test_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(train_vectors, train_classes):\n",
    "    logistic_regression_model = LogisticRegression(C=0.05, solver='liblinear', max_iter = 1000, penalty=\"l2\")\n",
    "    logistic_regression_model.fit(train_vectors, train_classes)\n",
    "    \n",
    "    return logistic_regression_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(model, test_vectors, test_classes):\n",
    "    accuracy = accuracy_score(test_classes, model.predict(test_vectors))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Accuracy Computation\n",
    "\n",
    "Below code computes the baseline accuracy after dividing the corpus into training and test dataset. This accuracy will be used to compare with the accuracies generated after summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_reviews()\n",
    "vectors, documents, classes = vectorize_input_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors, train_documents, train_classes, test_vectors, test_documents, test_classes = distribute_test_train_corpus(vectors, documents, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_accuracy:\t0.896\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_model = get_model(train_vectors, train_classes)\n",
    "baseline_accuracy = run_evaluation(logistic_regression_model, test_vectors, test_classes)\n",
    "\n",
    "print(\"baseline_accuracy:\\t{}\".format(baseline_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Text using Weighted Word Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(document, summary_sentence_count = 10, max_words_in_sentence = sys.maxsize):\n",
    "    original_text = document\n",
    "    # Preprocessing\n",
    "    formatted_text = re.sub(r'\\s+', ' ',  re.sub('[^a-zA-Z]', ' ', document))\n",
    "    # Converting Text To Sentences\n",
    "    sentence_list = sent_tokenize(document)\n",
    "    \n",
    "    # Find Weighted Frequency of Occurrence\n",
    "    stop_words = stopwords.words('english')\n",
    "    word_frequencies = {}\n",
    "    for word in word_tokenize(formatted_text):\n",
    "        if word not in stop_words:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "    \n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "    \n",
    "    # Calculating Sentence Scores\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentence_list:\n",
    "        if len(sentence.split(' ')) < max_words_in_sentence:\n",
    "            for word in word_tokenize(sentence.lower()):\n",
    "                if word in word_frequencies.keys():\n",
    "                    if sentence not in sentence_scores.keys():\n",
    "                        sentence_scores[sentence] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sentence] += word_frequencies[word]\n",
    "    \n",
    "    # Getting the Summary\n",
    "    summary_sentences = heapq.nlargest(summary_sentence_count, sentence_scores, key=sentence_scores.get)\n",
    "    summary = ' '.join(summary_sentences)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_corpus(documents, summary_sentence_count = 10, max_words_in_sentence = sys.maxsize):\n",
    "    document_summary = []\n",
    "    for index, document in enumerate(documents):\n",
    "        summary = get_summary(document, summary_sentence_count, max_words_in_sentence)\n",
    "        document_summary.append(summary)\n",
    "    \n",
    "    return document_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_summary(summary_corpus):\n",
    "    vectorizer = CountVectorizer(input=summary_corpus, max_df=0.25, token_pattern=r'\\b[a-zA-Z0-9]*[a-zA-Z][a-zA-Z0-9]*\\b', ngram_range=(1,3), max_features=300000, binary=True)\n",
    "    count_vector = vectorizer.fit_transform(summary_corpus)\n",
    "\n",
    "    summary_vectors = count_vector.toarray()\n",
    "    \n",
    "    return summary_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_summary_accuracy(train_documents, train_classes, test_documents, test_classes, summary_sentence_count = 10, max_words_in_sentence = sys.maxsize):\n",
    "    train_summary = create_summary_corpus(train_documents, summary_sentence_count, max_words_in_sentence)\n",
    "    test_summary = create_summary_corpus(test_documents, summary_sentence_count, max_words_in_sentence)\n",
    "    \n",
    "    summary_corpus = []\n",
    "    summary_corpus.extend(train_summary)\n",
    "    summary_corpus.extend(test_summary)\n",
    "\n",
    "    summary_vectors = vectorize_summary(summary_corpus)\n",
    "    train_summary_vectors = summary_vectors[0:1500]\n",
    "    test_summary_vectors = summary_vectors[1500:]\n",
    "\n",
    "    logistic_regression_model_after_summary = get_model(train_summary_vectors, train_classes)\n",
    "    weighted_average_summary_accuracy = run_evaluation(logistic_regression_model_after_summary, test_summary_vectors, test_classes)\n",
    "\n",
    "    return weighted_average_summary_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(document_summary):\t1500\n",
      "len(document_summary):\t500\n",
      "len(summary_corpus):\t2000\n",
      "len(train_summary_vectors):\t1500\n",
      "len(test_summary_vectors):\t500\n",
      "weighted_average_summary_accuracy:\t0.742\n"
     ]
    }
   ],
   "source": [
    "weighted_average_summary_accuracy = compute_summary_accuracy(train_documents, train_classes, test_documents, test_classes, summary_sentence_count = 7, max_words_in_sentence = 30)\n",
    "print(\"weighted_average_summary_accuracy:\\t{}\".format(weighted_average_summary_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(document_summary):\t1500\n",
      "len(document_summary):\t500\n",
      "len(summary_corpus):\t2000\n",
      "len(train_summary_vectors):\t1500\n",
      "len(test_summary_vectors):\t500\n",
      "weighted_average_summary_accuracy:\t0.762\n"
     ]
    }
   ],
   "source": [
    "weighted_average_summary_accuracy = compute_summary_accuracy(train_documents, train_classes, test_documents, test_classes, summary_sentence_count = 7)\n",
    "print(\"weighted_average_summary_accuracy:\\t{}\".format(weighted_average_summary_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(document_summary):\t1500\n",
      "len(document_summary):\t500\n",
      "len(summary_corpus):\t2000\n",
      "len(train_summary_vectors):\t1500\n",
      "len(test_summary_vectors):\t500\n",
      "weighted_average_summary_accuracy:\t0.784\n"
     ]
    }
   ],
   "source": [
    "weighted_average_summary_accuracy = compute_summary_accuracy(train_documents, train_classes, test_documents, test_classes)\n",
    "print(\"weighted_average_summary_accuracy:\\t{}\".format(weighted_average_summary_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
