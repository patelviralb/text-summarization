{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\viral\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import nltk\n",
    "import sys\n",
    "import re\n",
    "import heapq\n",
    "import gensim\n",
    "import sumy\n",
    "import timeit\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "from gensim.summarization import summarize\n",
    "\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_reviews():\n",
    "    raw = requests.get(\"https://raw.githubusercontent.com/patelviralb/text-summarization/main/dataset/cornell_reviews.json\").text.strip()\n",
    "    corpus = [json.loads(line) for line in raw.split(\"\\n\")]\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_input_corpus(documents):\n",
    "    documents = []\n",
    "    classes = []\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    for entry in corpus:\n",
    "        documents.append(entry['text'])\n",
    "        classes.append(entry['class'])\n",
    "\n",
    "    vectorizer = CountVectorizer(input=documents, max_df=0.25, token_pattern=r'\\b[a-zA-Z0-9]*[a-zA-Z][a-zA-Z0-9]*\\b', ngram_range=(1,3), max_features=300000, binary=True)\n",
    "    count_vector = vectorizer.fit_transform(documents)\n",
    "\n",
    "    vectors = count_vector.toarray()\n",
    "#     vocab = vectorizer.get_feature_names()\n",
    "\n",
    "#     return vectors, classes, vocab\n",
    "    return vectors, documents, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_test_train_corpus(vectors, documents, classes):\n",
    "    document_indices = [*range(0, len(documents), 1)]\n",
    "    test_train_data_indces = train_test_split(document_indices, train_size = 0.75, random_state = 41)\n",
    "\n",
    "    train_vectors = []\n",
    "    train_documents = []\n",
    "    train_classes = []\n",
    "\n",
    "    for index in test_train_data_indces[0]:\n",
    "        train_vectors.append(vectors[index])\n",
    "        train_documents.append(documents[index])\n",
    "        train_classes.append(classes[index])\n",
    "\n",
    "    test_vectors = []\n",
    "    test_documents = []\n",
    "    test_classes = []\n",
    "\n",
    "    for index in test_train_data_indces[1]:\n",
    "        test_vectors.append(vectors[index])\n",
    "        test_documents.append(documents[index])\n",
    "        test_classes.append(classes[index])\n",
    "    \n",
    "    return train_vectors, train_documents, train_classes, test_vectors, test_documents, test_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(train_vectors, train_classes):\n",
    "    logistic_regression_model = LogisticRegression(C=0.05, solver='liblinear', max_iter = 1000, penalty=\"l2\")\n",
    "    logistic_regression_model.fit(train_vectors, train_classes)\n",
    "    \n",
    "    return logistic_regression_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(model, test_vectors, test_classes):\n",
    "    accuracy = accuracy_score(test_classes, model.predict(test_vectors))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Accuracy Computation\n",
    "\n",
    "Below code computes the baseline accuracy after dividing the corpus into training and test dataset. This accuracy will be used to compare with the accuracies generated after summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_reviews()\n",
    "vectors, documents, classes = vectorize_input_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors, train_documents, train_classes, test_vectors, test_documents, test_classes = distribute_test_train_corpus(vectors, documents, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_accuracy:\t0.896\n",
      "Time:\t7.661498200000011\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "logistic_regression_model = get_model(train_vectors, train_classes)\n",
    "baseline_accuracy = run_evaluation(logistic_regression_model, test_vectors, test_classes)\n",
    "\n",
    "print(\"baseline_accuracy:\\t{}\".format(baseline_accuracy))\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"Time:\\t{}\".format((stop - start))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_summary(summary_corpus):\n",
    "    vectorizer = CountVectorizer(input=summary_corpus, max_df=0.25, token_pattern=r'\\b[a-zA-Z0-9]*[a-zA-Z][a-zA-Z0-9]*\\b', ngram_range=(1,3), max_features=300000, binary=True)\n",
    "    count_vector = vectorizer.fit_transform(summary_corpus)\n",
    "\n",
    "    summary_vectors = count_vector.toarray()\n",
    "    \n",
    "    return summary_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Text using `Weighted Word Frequency`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_word_summary(document, summary_sentence_count = 10, max_words_in_sentence = sys.maxsize):\n",
    "    original_text = document\n",
    "    # Preprocessing\n",
    "    formatted_text = re.sub(r'\\s+', ' ',  re.sub('[^a-zA-Z]', ' ', document))\n",
    "    # Converting Text To Sentences\n",
    "    sentence_list = sent_tokenize(document)\n",
    "    \n",
    "    # Find Weighted Frequency of Occurrence\n",
    "    stop_words = stopwords.words('english')\n",
    "    word_frequencies = {}\n",
    "    for word in word_tokenize(formatted_text):\n",
    "        if word not in stop_words:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "    \n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "    \n",
    "    # Calculating Sentence Scores\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentence_list:\n",
    "        if len(sentence.split(' ')) < max_words_in_sentence:\n",
    "            for word in word_tokenize(sentence.lower()):\n",
    "                if word in word_frequencies.keys():\n",
    "                    if sentence not in sentence_scores.keys():\n",
    "                        sentence_scores[sentence] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sentence] += word_frequencies[word]\n",
    "    \n",
    "    # Getting the Summary\n",
    "    summary_sentences = heapq.nlargest(summary_sentence_count, sentence_scores, key=sentence_scores.get)\n",
    "    summarized_text = ' '.join(summary_sentences)\n",
    "    \n",
    "    return summarized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weighted_word_summary_corpus(documents, summary_sentence_count = 10, max_words_in_sentence = sys.maxsize):\n",
    "    document_summary = []\n",
    "    for index, document in enumerate(documents):\n",
    "        summary = get_weighted_word_summary(document, summary_sentence_count, max_words_in_sentence)\n",
    "        document_summary.append(summary)\n",
    "    \n",
    "    return document_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_word_summary_accuracy(train_documents, train_classes, test_documents, test_classes, summary_sentence_count = 10, max_words_in_sentence = sys.maxsize):\n",
    "    train_summary = create_weighted_word_summary_corpus(train_documents, summary_sentence_count, max_words_in_sentence)\n",
    "    test_summary = create_weighted_word_summary_corpus(test_documents, summary_sentence_count, max_words_in_sentence)\n",
    "    \n",
    "    summary_corpus = []\n",
    "    summary_corpus.extend(train_summary)\n",
    "    summary_corpus.extend(test_summary)\n",
    "\n",
    "    summary_vectors = vectorize_summary(summary_corpus)\n",
    "    train_summary_vectors = summary_vectors[0:1500]\n",
    "    test_summary_vectors = summary_vectors[1500:]\n",
    "\n",
    "    logistic_regression_model_after_summary = get_model(train_summary_vectors, train_classes)\n",
    "    weighted_word_summary_accuracy = run_evaluation(logistic_regression_model_after_summary, test_summary_vectors, test_classes)\n",
    "\n",
    "    return weighted_word_summary_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted_word_summary_accuracy:\t0.742\n",
      "Time:\t23.066706600000003\n"
     ]
    }
   ],
   "source": [
    "# summary_sentence_count = 7, max_words_in_sentence = 30\n",
    "start = timeit.default_timer()\n",
    "\n",
    "weighted_word_summary_accuracy = compute_weighted_word_summary_accuracy(train_documents, train_classes, test_documents, test_classes, summary_sentence_count = 7, max_words_in_sentence = 30)\n",
    "print(\"weighted_word_summary_accuracy:\\t{}\".format(weighted_word_summary_accuracy))\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"Time:\\t{}\".format((stop - start))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted_word_summary_accuracy:\t0.762\n",
      "Time:\t24.48571610000002\n"
     ]
    }
   ],
   "source": [
    "# summary_sentence_count = 7\n",
    "start = timeit.default_timer()\n",
    "\n",
    "weighted_word_summary_accuracy = compute_weighted_word_summary_accuracy(train_documents, train_classes, test_documents, test_classes, summary_sentence_count = 7)\n",
    "print(\"weighted_word_summary_accuracy:\\t{}\".format(weighted_word_summary_accuracy))\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"Time:\\t{}\".format((stop - start))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted_word_summary_accuracy:\t0.784\n",
      "Time:\t25.41009250000002\n"
     ]
    }
   ],
   "source": [
    "# summary_sentence_count = 10\n",
    "start = timeit.default_timer()\n",
    "\n",
    "weighted_word_summary_accuracy = compute_weighted_word_summary_accuracy(train_documents, train_classes, test_documents, test_classes)\n",
    "print(\"weighted_word_summary_accuracy:\\t{}\".format(weighted_word_summary_accuracy))\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"Time:\\t{}\".format((stop - start))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Text using `TextRank`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_textrank_summary(original_text, ratio):\n",
    "    try:\n",
    "        summarized_text = summarize(original_text, ratio)\n",
    "    except ValueError as v:\n",
    "        return original_text\n",
    "\n",
    "    return summarized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_textrank_summary_corpus(documents, ratio):\n",
    "    document_summary = []\n",
    "    for index, document in enumerate(documents):\n",
    "        summary = get_textrank_summary(document, ratio)\n",
    "        document_summary.append(summary)\n",
    "    \n",
    "    return document_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_textrank_summary_accuracy(train_documents, train_classes, test_documents, test_classes, ratio=0.2):\n",
    "    train_summary = create_textrank_summary_corpus(train_documents, ratio)\n",
    "    if logging == True: print(\"train_summary completed\")\n",
    "    test_summary = create_textrank_summary_corpus(test_documents, ratio)\n",
    "    if logging == True: print(\"test_summary completed\")\n",
    "    \n",
    "    summary_corpus = []\n",
    "    summary_corpus.extend(train_summary)\n",
    "    summary_corpus.extend(test_summary)\n",
    "    if logging == True: print(\"summary_corpus created\")\n",
    "\n",
    "    summary_vectors = vectorize_summary(summary_corpus)\n",
    "    train_summary_vectors = summary_vectors[0:1500]\n",
    "    test_summary_vectors = summary_vectors[1500:]\n",
    "    if logging == True: print(\"summary_corpus distributed\")\n",
    "\n",
    "    logistic_regression_model_after_summary = get_model(train_summary_vectors, train_classes)\n",
    "    if logging == True: print(\"logistic_regression_model_after_summary computed\")\n",
    "    textrank_summary_accuracy = run_evaluation(logistic_regression_model_after_summary, test_summary_vectors, test_classes)\n",
    "    if logging == True: print(\"transformers_pipeline_summary_accuracy computed\")\n",
    "\n",
    "    return textrank_summary_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textrank_summary_accuray:\t0.742\n",
      "Time:\t26.61637509999997\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "textrank_summary_accuray = compute_textrank_summary_accuracy(train_documents, train_classes, test_documents, test_classes, 0.2)\n",
    "print(\"textrank_summary_accuray:\\t{}\".format(textrank_summary_accuray))\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"Time:\\t{}\".format((stop - start))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textrank_summary_accuray:\t0.792\n",
      "Time:\t22.217960800000014\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "textrank_summary_accuray = compute_textrank_summary_accuracy(train_documents, train_classes, test_documents, test_classes, 0.3)\n",
    "print(\"textrank_summary_accuray:\\t{}\".format(textrank_summary_accuray))\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"Time:\\t{}\".format((stop - start))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textrank_summary_accuray:\t0.818\n",
      "Time:\t24.45296239999999\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "textrank_summary_accuray = compute_textrank_summary_accuracy(train_documents, train_classes, test_documents, test_classes, 0.5)\n",
    "print(\"textrank_summary_accuray:\\t{}\".format(textrank_summary_accuray))\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"Time:\\t{}\".format((stop - start))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Text using `LexRank`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexrank_summary(original_text, sentence_count = 10):\n",
    "    \n",
    "    lexrank_parser = PlaintextParser.from_string(original_text,Tokenizer('english'))\n",
    "    \n",
    "    # Creating a summary of 3 sentences.\n",
    "    lex_rank_summarizer = LexRankSummarizer()\n",
    "    lexrank_summary = lex_rank_summarizer(lexrank_parser.document,sentences_count=sentence_count)\n",
    "\n",
    "    # Printing the summary\n",
    "    summarized_text = \"\"\n",
    "    for sentence in lexrank_summary:\n",
    "        summarized_text += str(sentence) + \" \"\n",
    "\n",
    "\n",
    "    return summarized_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lexrank_summary_corpus(documents, sentence_count = 10):\n",
    "    document_summary = []\n",
    "    for index, document in enumerate(documents):\n",
    "        summary = get_lexrank_summary(document, sentence_count)\n",
    "        document_summary.append(summary)\n",
    "    \n",
    "    return document_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lexrank_summary_accuracy(train_documents, train_classes, test_documents, test_classes, sentence_count = 10):\n",
    "    train_summary = create_lexrank_summary_corpus(train_documents, sentence_count)\n",
    "    if logging == True: print(\"train_summary completed\")\n",
    "    test_summary = create_textrank_summary_corpus(test_documents, sentence_count)\n",
    "    if logging == True: print(\"test_summary completed\")\n",
    "    \n",
    "    summary_corpus = []\n",
    "    summary_corpus.extend(train_summary)\n",
    "    summary_corpus.extend(test_summary)\n",
    "    if logging == True: print(\"summary_corpus created\")\n",
    "\n",
    "    summary_vectors = vectorize_summary(summary_corpus)\n",
    "    train_summary_vectors = summary_vectors[0:1500]\n",
    "    test_summary_vectors = summary_vectors[1500:]\n",
    "    if logging == True: print(\"summary_corpus distributed\")\n",
    "\n",
    "    logistic_regression_model_after_summary = get_model(train_summary_vectors, train_classes)\n",
    "    if logging == True: print(\"logistic_regression_model_after_summary computed\")\n",
    "    lexrank_summary_accuracy = run_evaluation(logistic_regression_model_after_summary, test_summary_vectors, test_classes)\n",
    "    if logging == True: print(\"transformers_pipeline_summary_accuracy computed\")\n",
    "\n",
    "    return lexrank_summary_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexrank_summary_accuray:\t0.84\n",
      "Time:\t62.40784009999999\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "lexrank_summary_accuray = compute_lexrank_summary_accuracy(train_documents, train_classes, test_documents, test_classes, 7)\n",
    "print(\"lexrank_summary_accuray:\\t{}\".format(lexrank_summary_accuray))\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"Time:\\t{}\".format((stop - start))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexrank_summary_accuray:\t0.852\n",
      "Time:\t60.62418530000002\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "lexrank_summary_accuray = compute_lexrank_summary_accuracy(train_documents, train_classes, test_documents, test_classes)\n",
    "print(\"lexrank_summary_accuray:\\t{}\".format(lexrank_summary_accuray))\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"Time:\\t{}\".format((stop - start))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "\n",
    "lexrank_summary_accuray = compute_lexrank_summary_accuracy(train_documents, train_classes, test_documents, test_classes, 2)\n",
    "print(\"lexrank_summary_accuray:\\t{}\".format(lexrank_summary_accuray))\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"Time:\\t{}\".format((stop - start))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Text using `transformers.pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformers_pipeline_summary(original_text, minimum_length = 20, maximum_length = 200):\n",
    "    original_text_length = len(original_text)\n",
    "    \n",
    "    summarization = pipeline(\"summarization\")\n",
    "    \n",
    "    original_documents = []\n",
    "    if original_text_length > 1024:\n",
    "        total_range = ceil(original_text_length / 1024)\n",
    "        i = 0\n",
    "        while i < total_range:\n",
    "            start = i * 1024\n",
    "            end = (start + 1024) if i != total_range - 1 else original_text_length\n",
    "            summarized_text = summarization(original_text[start:end], min_length = minimum_length, max_length = original_text_length if maximum_length > original_text_length else maximum_length)[0]['summary_text']\n",
    "            original_documents.append(summarized_text)\n",
    "            i += 1\n",
    "        \n",
    "        return \" \".join(original_documents)\n",
    "    \n",
    "    summarized_text = summarization(original_text, min_length = minimum_length, max_length = original_text_length if maximum_length > original_text_length else maximum_length)[0]['summary_text']\n",
    "    \n",
    "    return summarized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformers_pipeline_summary_corpus(documents, min_length = 20, max_length = 200):\n",
    "    document_summary = []\n",
    "    for index, document in enumerate(documents):\n",
    "        summary = get_transformers_pipeline_summary(document, min_length, max_length)\n",
    "        document_summary.append(summary)\n",
    "    \n",
    "    return document_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transformers_pipeline_summary_accuracy(train_documents, train_classes, test_documents, test_classes, min_length = 20, max_length = 200):\n",
    "    train_summary = create_transformers_pipeline_summary_corpus(train_documents, min_length, max_length)\n",
    "    if logging == True: print(\"train_summary completed\")\n",
    "    test_summary = create_transformers_pipeline_summary_corpus(test_documents, min_length, max_length)\n",
    "    if logging == True: print(\"test_summary completed\")\n",
    "    \n",
    "    summary_corpus = []\n",
    "    summary_corpus.extend(train_summary)\n",
    "    summary_corpus.extend(test_summary)\n",
    "    if logging == True: print(\"summary_corpus created\")\n",
    "\n",
    "    summary_vectors = vectorize_summary(summary_corpus)\n",
    "    train_summary_vectors = summary_vectors[0:1500]\n",
    "    test_summary_vectors = summary_vectors[1500:]\n",
    "    if logging == True: print(\"summary_corpus distributed\")\n",
    "\n",
    "    logistic_regression_model_after_summary = get_model(train_summary_vectors, train_classes)\n",
    "    if logging == True: print(\"logistic_regression_model_after_summary computed\")\n",
    "    transformers_pipeline_summary_accuracy = run_evaluation(logistic_regression_model_after_summary, test_summary_vectors, test_classes)\n",
    "    if logging == True: print(\"transformers_pipeline_summary_accuracy computed\")\n",
    "\n",
    "    return transformers_pipeline_summary_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_length = 5, max_length = 20\n",
    "start = timeit.default_timer()\n",
    "\n",
    "transformers_pipeline_summary_accuracy = compute_transformers_pipeline_summary_accuracy(train_documents, train_classes, test_documents, test_classes, 5, 20)\n",
    "print(\"transformers_pipeline_summary_accuracy:\\t{}\".format(transformers_pipeline_summary_accuracy))\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"Time:\\t{}\".format((stop - start))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_length = 0, max_length = 200\n",
    "start = timeit.default_timer()\n",
    "\n",
    "transformers_pipeline_summary_accuracy = compute_transformers_pipeline_summary_accuracy(train_documents, train_classes, test_documents, test_classes, 0, 200)\n",
    "print(\"transformers_pipeline_summary_accuracy:\\t{}\".format(transformers_pipeline_summary_accuracy))\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print(\"Time:\\t{}\".format((stop - start))) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
